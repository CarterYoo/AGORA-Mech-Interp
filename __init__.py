"""
AGORA Mechanistic Interpretability Research

A research framework for analyzing and mitigating hallucinations in 
Retrieval-Augmented Generation systems using mechanistic interpretability 
techniques, specifically designed for AI governance and policy documents.

This implementation follows:
- RAGTruth methodology for hallucination annotation
- Logit Lens analysis for layer-wise interpretation
- ReDEeP-inspired mechanistic interpretability techniques
"""

__version__ = "0.1.0"
__author__ = "AGORA MI Research Team"

